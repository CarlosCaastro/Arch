version: '3'

x-spark-common: &spark-common
  build:
    context: .
    dockerfile: Dockerfile_spark
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
    - ./warehouse:/opt/bitnami/spark/warehouse
    - ./data:/opt/data
  networks:
    - code-carlos

# x-airflow-common: &airflow-common
#   build:
#     context: .
#     dockerfile: Dockerfile
#   env_file:
#     - airflow.env
#   volumes:
#     - ./jobs:/opt/airflow/jobs
#     - ./dags:/opt/airflow/dags
#     - ./logs:/opt/airflow/logs
#     - ./data:/opt/data
#   depends_on:
#     - postgres
#   networks:
#     - code-carlos

services:

  # postgres:
  #   image: postgres:16
  #   container_name: hive-metastore-postgres
  #   environment:
  #     POSTGRES_USER: hive
  #     POSTGRES_PASSWORD: hivepass123
  #     POSTGRES_DB: hive_metastore
  #     PGDATA: /var/lib/postgresql/data/pgdata
  #   ports:
  #     - "5432:5432"
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U hive"]
  #     interval: 5s
  #     retries: 5
  #   volumes:
  #     - postgres-data:/var/lib/postgresql/data
  #   networks:
  #     - code-carlos

  # hive-metastore:
  #   image: apache/hive:4.0.0-alpha-2
  #   container_name: hive-metastore
  #   environment:
  #     SERVICE_NAME: metastore
  #     DB_DRIVER: postgres
  #     SERVICE_OPTS: >
  #       -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
  #       -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/hive_metastore
  #       -Djavax.jdo.option.ConnectionUserName=hive
  #       -Djavax.jdo.option.ConnectionPassword=hivepass123
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   ports:
  #     - "9083:9083"
  #     - "10000:10000"
  #   volumes:
  #     - ./warehouse:/opt/bitnami/spark/warehouse
  #     - ./hive-conf/hive-site.xml:/opt/hive/conf/hive-site.xml
  #   command: >
  #     bash -c "
  #     sleep 10 && 
  #     /opt/hive/bin/schematool -dbType postgres -initSchema &&
  #     /opt/hive/bin/hive --service metastore
  #     "
  #   networks:
  #     - code-carlos

  spark-master:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    environment:
      PYSPARK_PYTHON: /usr/local/bin/python3.11
      PYSPARK_DRIVER_PYTHON: /usr/local/bin/python3.11
    # depends_on:
    #   - hive-metastore

  spark-worker:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "9091:8081"
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
      PYSPARK_PYTHON: /usr/local/bin/python3.11
      PYSPARK_DRIVER_PYTHON: /usr/local/bin/python3.11
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/data
      - ./warehouse:/opt/bitnami/spark/warehouse
      
  # postgres:
  #   image: postgres:14.0
  #   environment:
  #     - POSTGRES_USER=airflow
  #     - POSTGRES_PASSWORD=airflow
  #     - POSTGRES_DB=airflow
  #   ports:
  #     - "5432:5432"
  #   networks:
  #     - code-carlos

  # webserver:
  #   <<: *airflow-common
  #   command: bash -c "python /opt/airflow/dags/scripts/connections.py && airflow webserver"
  #   ports:
  #     - "8080:8080"
  #   depends_on:
  #     - postgres
  #     - scheduler

  # scheduler:
  #   <<: *airflow-common
  #   command: bash -c "airflow db migrate && airflow users create --username airflow --firstname Carlos --lastname Castro --role Admin --email carlos.candradr@gmail.com --password airflow && airflow scheduler"

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    volumes:
      - ./minio/data:/data
    networks:
      - code-carlos

volumes:
  postgres-data:
  data:
  warehouse:

networks:
  code-carlos:
    driver: bridge